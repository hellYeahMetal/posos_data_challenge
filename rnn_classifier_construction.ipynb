{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from utils import train_test_validation_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, Flatten, LSTM\n",
    "from keras.losses import sparse_categorical_crossentropy, categorical_hinge\n",
    "from keras import optimizers\n",
    "from keras.layers import Masking, TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28, 31, 44, 48, 22, 23, 42, 32, 26,  0, 34, 14,  7, 37,  4, 11, 24,\n",
       "       40, 46, 30,  8, 38, 13, 21, 15, 27,  5, 33,  6, 25,  1, 50, 43, 45,\n",
       "       39, 29, 19, 12, 47, 20,  9, 10, 41, 49, 18, 17,  2, 36, 16, 35,  3])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_csv('DATA/clean_input_train.csv', sep=\";\", index_col=0)\n",
    "y = pd.read_csv('DATA/output_train.csv', sep=\";\", index_col=0)\n",
    "\n",
    "features = X.columns\n",
    "targets = y['intention'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimation of the size of the vocabulary \n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit_transform(X['question'])\n",
    "MAX_NB_WORDS = len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find max size of the sequences \n",
    "MAX_SEQUENCE_LENGTH = 0 \n",
    "for sentence in X['question']:\n",
    "    if MAX_SEQUENCE_LENGTH<len(sentence.split()):\n",
    "        MAX_SEQUENCE_LENGTH = len(sentence.split())\n",
    "MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8830 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Preprocess text fo feed the net \n",
    "texts = X['question']\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS/2)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "X_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_validation, y_train, y_test, y_validation = train_test_validation_split(X_sequences,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 3888,  643, 1209],\n",
       "       [   0,    0,    0, ...,  132,  250,   13],\n",
       "       [   0,    0,    0, ..., 1393,   96,   41],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  240,  463,  368],\n",
       "       [   0,    0,    0, ...,    3,    2,    1],\n",
       "       [   0,    0,    0, ...,   24,   10,   29]], dtype=int32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parameters \n",
    "EMBEDDING_DIM = 100\n",
    "NB_CATEGORIES = len(targets)\n",
    "#Define optimizer \n",
    "optz = optimizers.RMSprop(lr=0.006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_32 (Embedding)     (None, 412, 100)          883100    \n",
      "_________________________________________________________________\n",
      "lstm_34 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 51)                5151      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 51)                0         \n",
      "=================================================================\n",
      "Total params: 968,651\n",
      "Trainable params: 968,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 5137 samples, validate on 1285 samples\n",
      "Epoch 1/3\n",
      "5137/5137 [==============================] - 69s 13ms/step - loss: 2.7621 - acc: 0.3093 - val_loss: 2.3256 - val_acc: 0.4093\n",
      "Epoch 2/3\n",
      "5137/5137 [==============================] - 75s 15ms/step - loss: 1.8105 - acc: 0.5198 - val_loss: 1.8638 - val_acc: 0.5082\n",
      "Epoch 3/3\n",
      "5137/5137 [==============================] - 66s 13ms/step - loss: 1.2435 - acc: 0.6710 - val_loss: 1.7009 - val_acc: 0.5556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efce8e2ce10>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model \n",
    "model_2 = Sequential()\n",
    "model_2.add(Embedding(len(word_index)+1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True))\n",
    "model_2.add(LSTM(100))\n",
    "model_2.add((Dense(NB_CATEGORIES)))\n",
    "model_2.add(Activation('softmax')) # reminder sigmoid if is for binary classification\n",
    "model_2.compile(loss=sparse_categorical_crossentropy, optimizer=optz, metrics=['accuracy'])\n",
    "print(model_2.summary())\n",
    "model_2.fit(X_train, y_train.values, validation_data=(X_test, y_test.values), epochs=3, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
