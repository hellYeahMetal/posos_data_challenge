{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from utils import train_test_validation_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, Flatten, LSTM, GRU\n",
    "from keras.losses import sparse_categorical_crossentropy, categorical_hinge\n",
    "from keras import optimizers\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "X = pd.read_csv('DATA/clean_input_train.csv', sep=\";\", index_col=0)\n",
    "y = pd.read_csv('DATA/output_train.csv', sep=\";\", index_col=0)\n",
    "\n",
    "features = X.columns\n",
    "targets = y['intention'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['question'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for nn and find correct parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimation of the size of the vocabulary \n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit_transform(X['question'])\n",
    "MAX_NB_WORDS = len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find max size of the sequences \n",
    "MAX_SEQUENCE_LENGTH = 0 \n",
    "for sentence in X['question']:\n",
    "    if MAX_SEQUENCE_LENGTH<len(sentence.split()):\n",
    "        MAX_SEQUENCE_LENGTH = len(sentence.split())\n",
    "MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text fo feed the net \n",
    "texts = X['question']\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS/2)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "X_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_validation, y_train, y_test, y_validation = train_test_validation_split(X_sequences,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 3888,  643, 1209],\n",
       "       [   0,    0,    0, ...,  132,  250,   13],\n",
       "       [   0,    0,    0, ..., 1393,   96,   41],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  240,  463,  368],\n",
       "       [   0,    0,    0, ...,    3,    2,    1],\n",
       "       [   0,    0,    0, ...,   24,   10,   29]], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unique_logpath(logdir, raw_run_name):\n",
    "        i = 0\n",
    "        while(True):\n",
    "                run_name = raw_run_name + \"-\" + str(i)\n",
    "                log_path = os.path.join(logdir, run_name)\n",
    "                if not os.path.isdir(log_path):\n",
    "                        return log_path\n",
    "                i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parameters \n",
    "EMBEDDING_DIM = 200\n",
    "NB_CATEGORIES = len(targets)\n",
    "NB_LSTM = 50\n",
    "PERC_DROPOUT = 0.5\n",
    "EPOCHS = 10\n",
    "#Define RMSProp optimizer\n",
    "LEARNING_RATE = 0.006\n",
    "RATE_DECAY = LEARNING_RATE / EPOCHS\n",
    "optz = optimizers.RMSprop(lr=LEARNING_RATE, decay=RATE_DECAY)\n",
    "\n",
    "\n",
    "sgd = optimizers.SGD(lr=LEARNING_RATE, decay=RATE_DECAY, momentum=0.9, nesterov=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"gru_\"+str(NB_LSTM)+\"_rmsprop_\"+str(LEARNING_RATE)\n",
    "run_name += \"_decay_embedding_\"+str(EMBEDDING_DIM)\n",
    "run_name +=\"_dropout_\"+str(PERC_DROPOUT)+\"_early_stop_shuffle\"\n",
    "\n",
    "logpath = generate_unique_logpath(\"./logs_tensorboard\", run_name)\n",
    "tbcb = TensorBoard(log_dir=logpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 412, 200)          1766200   \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 50)                37650     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 51)                2601      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 51)                0         \n",
      "=================================================================\n",
      "Total params: 1,806,451\n",
      "Trainable params: 1,806,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 5137 samples, validate on 1285 samples\n",
      "Epoch 1/10\n",
      "5137/5137 [==============================] - 62s 12ms/step - loss: 2.9289 - acc: 0.2897 - val_loss: 2.3886 - val_acc: 0.3860\n",
      "Epoch 2/10\n",
      "5137/5137 [==============================] - 57s 11ms/step - loss: 2.0240 - acc: 0.4867 - val_loss: 1.9442 - val_acc: 0.5152\n",
      "Epoch 3/10\n",
      "5137/5137 [==============================] - 55s 11ms/step - loss: 1.5443 - acc: 0.6081 - val_loss: 1.7930 - val_acc: 0.5447\n",
      "Epoch 4/10\n",
      "5137/5137 [==============================] - 51s 10ms/step - loss: 1.1992 - acc: 0.6990 - val_loss: 1.7160 - val_acc: 0.5813\n",
      "Epoch 5/10\n",
      "5137/5137 [==============================] - 45s 9ms/step - loss: 0.9566 - acc: 0.7621 - val_loss: 1.6927 - val_acc: 0.5977\n",
      "Epoch 6/10\n",
      "5137/5137 [==============================] - 55s 11ms/step - loss: 0.7603 - acc: 0.8098 - val_loss: 1.7480 - val_acc: 0.6000\n",
      "Epoch 7/10\n",
      "5137/5137 [==============================] - 50s 10ms/step - loss: 0.6438 - acc: 0.8443 - val_loss: 1.8064 - val_acc: 0.5938\n",
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feb53818b00>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model \n",
    "model= Sequential()\n",
    "model.add(Embedding(len(word_index)+1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True))\n",
    "model.add(GRU(NB_LSTM))\n",
    "model.add(Dropout(PERC_DROPOUT))\n",
    "model.add((Dense(NB_CATEGORIES)))\n",
    "model.add(Activation('softmax')) # reminder sigmoid if is for binary classification\n",
    "model.compile(loss=sparse_categorical_crossentropy, optimizer=optz, metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, verbose=1,  mode='auto')\n",
    "checkpoint_filepath = os.path.join(logpath,  \"model.h1\")\n",
    "checkpoint_cb = ModelCheckpoint(checkpoint_filepath, save_best_only=True)\n",
    "model.fit(X_train, y_train.values,\n",
    "            validation_data=(X_test, y_test.values), \n",
    "            epochs=EPOCHS,\n",
    "            shuffle=True,\n",
    "            batch_size=64,\n",
    "            verbose=1,\n",
    "            callbacks=[tbcb, early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
